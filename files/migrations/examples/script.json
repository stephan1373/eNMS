[
  {
    "category": "Profiling",
    "code": "# The data can be sorted by:\n# - \"count\": how many times a function was called\n# - \"average_time\": how much time does the function take on average\n# - \"combined_time\": how much time did the function take in total\nSORT_BY = \"count\"\n\n# The data can be filtered by origin:\n# - If the origin is \"\", all the data is displayed\n# - Possible origins include: \"Environment\", \"Controller\",\n# \"Runner\", \"CustomApp\", \"RestApi\"\nORIGIN = \"\"\n\n# Number of entries to display\n# By default, we only display the 20 largest entries\nLIMIT = 20\n\ndata = sorted(\n    [\n        (name, entry)\n        for name, entry in vs.profiling.items()\n        if entry[\"class\"].startswith(ORIGIN)\n    ],\n    key=lambda item: item[1][SORT_BY],\n    reverse=True,\n)[:LIMIT]\n\nprint(\n    \"\\n\\n\".join(\n        f\"Function '{name}':\\n {vs.dict_to_string(data, depth=2)}\"\n        for name, data in data\n    )\n)\n",
    "creation_time": "2025-07-25 11:49:14.990787",
    "creator": "admin",
    "description": "Sort and display the profiling data stored in vs.profiling",
    "id": 1,
    "last_modified": "2025-07-26 15:26:50.963870",
    "last_modified_by": "admin",
    "name": "Display Profiling Data",
    "version": ""
  },
  {
    "category": "Database Tests",
    "code": "# Factory: create a new instance\n\ndevice = db.factory(\"device\", name=\"test\", model=\"IOS\", commit=True)\nprint(f\"Device '{device}' has been created\")\n\n# Fetch: retrieve instance from database\n\ndevice = db.fetch(\"device\", name=\"test\")\nprint(f\"Device '{device}' retrieved from database\")\n\n# Factory: edit existing instance\n# We use factory to update the \"model\" property of Washington\n\nprint(f\"Old model: {device.model}\")\ndevice = db.factory(\"device\", name=\"test\", model=\"EOS\")\nprint(f\"New model: {device.model}\")\n\n# Delete: erase instance from database\n\ndb.delete(\"device\", name=\"test\")\ndb.session.commit()\n\nget_deleted_device = db.fetch(\"device\", name=\"test\", allow_none=True)\n\nif get_deleted_device is None:\n    print(\"Device has been deleted\")\n",
    "creation_time": "2025-07-26 09:08:08.597839",
    "creator": "admin",
    "description": "Tests factory (for creating and editing), fetch, and delete",
    "id": 2,
    "last_modified": "2025-07-26 15:29:30.909815",
    "last_modified_by": "admin",
    "name": "Functions provided by eNMS",
    "version": ""
  },
  {
    "category": "Scalability",
    "code": "from pathlib import Path\n\npath_folder = vs.file_path / \"test\"\npath_folder.mkdir(exist_ok=True)\n\nfor i in range(10000):\n    file_path = path_folder / f\"file{i}\"\n    file_path.touch()\n",
    "creation_time": "2025-07-26 09:11:08.608881",
    "creator": "admin",
    "description": "This script is used to optimize the \"scan_folder\" function.\nIt creates 10K files in a subfolder \"test\" of the \"files\" folder",
    "id": 3,
    "last_modified": "2025-07-26 15:29:46.001197",
    "last_modified_by": "admin",
    "name": "Create 10K files",
    "version": ""
  },
  {
    "category": "Query Monitoring",
    "code": "db.monitor_orm_statements = True\ndb.orm_statements.clear()\ndb.orm_statements_runtime.clear()\ndb.orm_statements_tracebacks.clear()\n",
    "creation_time": "2025-07-26 09:12:38.966690",
    "creator": "admin",
    "description": "This script starts the monitoring mechanism for SQL queries and clears the content of the dict used for the analysis.",
    "id": 4,
    "last_modified": "2025-07-26 15:29:13.826759",
    "last_modified_by": "admin",
    "name": "Start Monitoring",
    "version": ""
  },
  {
    "category": "Scalability",
    "code": "from itertools import batched\n\nentry = (\"changelog\", \"content\", \"admin\", vs.get_time())\nquery = \"INSERT INTO changelog (type, content, author, time) VALUES (?, ?, ?, ?)\"\nbatch_size = vs.database[\"transactions\"][\"batch_size\"]\nlog_size = 50_000_000\n\nenv.log(\"info\", f\"Starting to create Fake Changelogs ({log_size} logs)\")\n\nwith env.timer(\"Create Fake Changelogs\"):\n    changelogs = (entry for _ in range(log_size))\n    cursor = db.session.connection().connection.cursor()\n    for batch in batched(changelogs, batch_size):\n        cursor.executemany(query, batch)\n    db.session.commit()\n",
    "creation_time": "2025-07-26 09:14:56.501843",
    "creator": "admin",
    "description": "Uses low-level SQL to create many changelogs (50M by default)",
    "id": 5,
    "last_modified": "2025-07-26 15:29:55.729400",
    "last_modified_by": "admin",
    "name": "Create 50M Changelogs",
    "version": ""
  },
  {
    "category": "Scalability",
    "code": "from datetime import datetime\n\ndevice_number = 200\n\nfactory_time = datetime.now()\nfor index in range(device_number):\n    db.factory(\"device\", name=str(index), commit=True)\nprint(f\"'db.factory' time: {datetime.now() - factory_time}\")\n\nfetch_time = datetime.now()\nfor index in range(device_number):\n    db.fetch(\"device\", name=str(index))\nprint(f\"'db.fetch' time: {datetime.now() - fetch_time}\")\n\ndelete_time = datetime.now()\nfor index in range(device_number):\n    db.delete(\"device\", name=str(index))\n    db.session.commit()\nprint(f\"'db.delete' time: {datetime.now() - delete_time}\")\n\nprint(f\"Total duration: {datetime.now() - factory_time}\")\n",
    "creation_time": "2025-07-26 09:15:33.006728",
    "creator": "admin",
    "description": "Uses eNMS fetch, factory and delete function to measure eNMS performances when querying the database.",
    "id": 6,
    "last_modified": "2025-07-26 15:29:50.911662",
    "last_modified_by": "admin",
    "name": "Tests fetch, factory and delete performance",
    "version": ""
  },
  {
    "category": "Database Cleanup",
    "code": "for service in db.fetch_all(\"service\", shared=False):\n    if service.workflows or service.name == service.scoped_name:\n        continue\n    print(f\"Deleting '{service}'\")\n    db.session.delete(service)\ndb.session.commit()\n",
    "creation_time": "2025-07-26 12:18:25.993536",
    "creator": "admin",
    "description": "Corrupted services are services that used to belong to a workflow and were removed from the workflow relationship without being deleted.\nThey have the following characteristics:\n- They don't belong to a workflow\n- They are not shared\n- Their scoped name differ from their name",
    "id": 7,
    "last_modified": "2025-07-26 15:27:43.729597",
    "last_modified_by": "admin",
    "name": "Delete corrupted services",
    "version": ""
  },
  {
    "category": "Database Cleanup",
    "code": "from collections import defaultdict\n\nedges = set(db.fetch_all(\"workflow_edge\"))\nduplicated_edges, number_of_corrupted_edges = defaultdict(list), 0\nfor edge in list(edges):\n    services = getattr(edge.workflow, \"services\", [])\n    if (\n        not edge.source\n        or not edge.destination\n        or not edge.workflow\n        or edge.source not in services\n        or edge.destination not in services\n        or edge.soft_deleted\n    ):\n        edges.remove(edge)\n        db.session.delete(edge)\n        number_of_corrupted_edges += 1\ndb.session.commit()\nfor edge in edges:\n    duplicated_edges[\n        (\n            edge.source.name,\n            edge.destination.name,\n            edge.workflow.name,\n            edge.subtype,\n        )\n    ].append(edge)\nfor duplicates in duplicated_edges.values():\n    for duplicate in duplicates[1:]:\n        db.session.delete(duplicate)\n        number_of_corrupted_edges += 1\n\nprint(f\"Number of corrupted edges deleted: {number_of_corrupted_edges}\")\n",
    "creation_time": "2025-07-26 12:19:21.777448",
    "creator": "admin",
    "description": "Corrupted edges can be:\n- edges missing a source, destinaton or a workflow\n- edges whose source or destination is not in the workflow's services\n- duplicated edges i.e with same source, destination, workflow and subtype",
    "id": 8,
    "last_modified": "2025-07-26 15:27:47.855430",
    "last_modified_by": "admin",
    "name": "Delete corrupted edges",
    "version": ""
  },
  {
    "category": "Memory",
    "code": "from sys import getsizeof\n\nVARIABLES = {\n    \"connections_cache\": \"Network Connections Cache\",\n    \"service_run_count\": \"Service Run cache\",\n    \"run_states\": \"Run State Cache\",\n    \"run_logs\": \"Run Logs Cache\",\n}\n\nprint(\"MEMORY USAGE :\", end=\"\\n\\n\")\nfor variable, function in VARIABLES.items():\n    size = getsizeof(getattr(app, variable))\n    print(f\"{function} ({variable}): {size} bytes\")\n",
    "creation_time": "2025-07-26 14:18:08.306994",
    "creator": "admin",
    "description": "This script monitors the memory usage of a few controller variables used for storing data, like the network connections cache (stores netmiko, napalm and scrapli connections) and the logs cache (stores service logs).\nMemory usage of these variables should not grow over time are their content is supposed to be deleted when no longer useful.",
    "id": 9,
    "last_modified": "2025-07-26 15:28:32.540100",
    "last_modified_by": "admin",
    "name": "Monitor memory usage",
    "version": ""
  },
  {
    "category": "Memory",
    "code": "from operator import itemgetter\nfrom pympler import muppy, summary\n\n\nmax_results = 3\nall_objects = muppy.get_objects()\n\n\ndef format_size(size):\n    for unit in [\"B\", \"KiB\", \"MiB\", \"GiB\"]:\n        if size < 1024.0 or unit == \"GiB\":\n            break\n        size /= 1024.0\n    return f\"{size:.2f} {unit}\"\n\n\nprofile = sorted(\n    (object_ for object_ in summary.summarize(all_objects)),\n    key=itemgetter(2),\n    reverse=True,\n)\n\nfor object_ in profile[:max_results]:\n    print(f\"Name: {object_[0]}\")\n    print(f\"Number of objects: {object_[1]}\")\n    print(f\"Total size: {format_size(object_[2])}\", end=\"\\n\\n\")\n\n\nfor type in (str, dict):\n    print(f\"Last {max_results} {type} objects in memory\", end=\"\\n\\n\")\n    latest_objects = muppy.filter(all_objects, Type=type)[-max_results:]\n    print(\"\\n\".join(map(str, latest_objects)))\n",
    "creation_time": "2025-07-26 14:19:23.331444",
    "creator": "admin",
    "description": "Uses the \"pympler\" module for profiling memory. By default, objects are sorted by their size in memory.\nThe order can be changed to sorting by number of objects in memory by replacing \"itemgetter(2)\" with \"itemgetter(1)\".\nThe \"max_results\" variable limits the output to the first max_results results.",
    "id": 10,
    "last_modified": "2025-07-26 15:28:43.992949",
    "last_modified_by": "admin",
    "name": "Profiling with pymler",
    "version": ""
  },
  {
    "category": "Query Monitoring",
    "code": "print(f\"Number of Queries: {db.orm_statements.total()}\\n\")\n\nfor query, count in db.orm_statements.most_common():\n    print(f\"{count}: {query}\\n\")\n    query_time = db.orm_statements_runtime[query]\n    print(f\"Execution time ({count} query): {query_time}\\n\")\n    tracebacks = \"\\n\\n\".join(\n        f\"{count}: {traceback}\"\n        for traceback, count in db.orm_statements_tracebacks[query].items()\n    )\n    print(f\"Tracebacks:\\n{tracebacks}\\n\\n\")\n\ndb.monitor_orm_statements = False\n",
    "creation_time": "2025-07-26 14:22:25.418046",
    "creator": "admin",
    "description": "This script use the SQLAlchemy _do_orm_execute event to analyze what SQL queries were executed (SQL statement) and how much time each query took.",
    "id": 11,
    "last_modified": "2025-07-26 15:29:02.708790",
    "last_modified_by": "admin",
    "name": "End Monitoring",
    "version": ""
  },
  {
    "category": "Profiling",
    "code": "vs.profiling = {}\n",
    "creation_time": "2025-07-26 14:22:56.470569",
    "creator": "admin",
    "description": "Empty the profiling data dictionary",
    "id": 12,
    "last_modified": "2025-07-26 15:27:25.933840",
    "last_modified_by": "admin",
    "name": "Empty Data",
    "version": ""
  },
  {
    "category": "Database Tests",
    "code": "pool = db.factory(\"pool\", name=\"A pool\")\npool.devices = [db.fetch(\"device\", name=\"Washington\") for _ in range(5)]\ndb.session.commit()\n",
    "creation_time": "2025-07-26 14:25:50.993599",
    "creator": "admin",
    "description": "Provides a straightforward way to reproduce the duplicate rows issue in an association table.\nPrevents deleting both part of the association.\nExpected error: \"sqlalchemy.orm.exc.StaleDataError: DELETE statement on table 'pool_device_association' expected to delete 1 row(s); Only 5 were matched.\"",
    "id": 13,
    "last_modified": "2025-07-26 15:29:36.529611",
    "last_modified_by": "admin",
    "name": "Trigger Duplicates Error",
    "version": ""
  },
  {
    "category": "Database Cleanup",
    "code": "controller.delete_soft_deleted_objects()\nprint(f\"Soft-deleted objects successfully deleted\")\n",
    "creation_time": "2025-07-26 14:26:32.632192",
    "creator": "admin",
    "description": "Delete all soft-deleted workflow edges and services",
    "id": 14,
    "last_modified": "2025-07-26 15:27:31.441771",
    "last_modified_by": "admin",
    "name": "Delete Soft-Deleted Objects",
    "version": ""
  },
  {
    "category": "Scalability",
    "code": "from random import choices\nfrom sqlalchemy import text\nfrom string import ascii_lowercase\n\nconfiguration = \"\\n\".join(\n  \"\".join(choices(ascii_lowercase, k=50))\n  for _ in range(1_000)\n)\n\ndb.session.execute(\n    text(\"UPDATE device SET configuration = :config\"),\n    {\"config\": configuration}\n)\ndb.session.commit()",
    "creation_time": "2025-07-26 16:57:04.606507",
    "creator": "admin",
    "description": "We create a configuration of 1K lines, 50 characters per line.\nThen we set all devices to that configuration with low-level SQL.",
    "id": 15,
    "last_modified": "2025-07-26 18:35:25.150912",
    "last_modified_by": "admin",
    "name": "Set Large Configurations",
    "version": ""
  }
]